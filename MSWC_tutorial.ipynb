{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGm4fad3M-Sr"
   },
   "source": [
    "# MSWC FSCIL NeuroBench Tutorial\n",
    "\n",
    "This tutorial aims to provide an insight on the MSWC FSCIL NeuroBench task and present how you can use the corresponding NeuroBench harness to benchmark your own models and solutions! In particular we give a tutorial to implement the prototypical network approach to both a convolutional and a recurrent spiking network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### About FSCIL (Few-Shot Class-Incremental Learning)\n",
    "\n",
    "Learning new tasks from a small amount of experiences while retaining knowledge of prior tasks is a hallmark of biological intelligence and a long-standing goal of general AI. It is especially a key challenge to endow edge devices with the ability to adapt to their environments and users. This benchmark thus evaluates the capacity of a learning solution to successively incorporate new classes over multiple sessions (class-incremental), with only a handful of samples from the new classes to train with (few-shot). The FSCIL task is a recently established benchmark in the computer vision domain (https://arxiv.org/abs/2004.10956), but it has not yet been adapted to other data modalities. \n",
    "\n",
    "### The MSWC FSCIL NeuroBench Task:\n",
    "Aligning with a neuromorphic interest in temporal data modalities, this benchmark introduces a FSCIL task for streaming audio keyword classification using the large Multilingual Spoken Word Corpus (MSWC) dataset (https://mlcommons.org/datasets/multilingual-spoken-words/). The task is designed to be approached in two phases: pre-training and incremental learning:\n",
    "* First, for pre-training, a set of 100 words spanning 5 base languages (English, German, Catalan, French, Kinyarwanda) with 500 training samples each are made available to train an initial model. We provide here 2 pre-trained models, a convolutional and a recurrent spiking one, both trained with gradient descent on the train samples of the 100 base keywords.\n",
    "\n",
    "* Next, for incremental learning, the model undergoes 10 successive sessions to learn words from 10 new languages (Persian, Spanish, Russian, Welsh, Italian, Basque, Polish, Esparanto, Portuguese, Dutch) in a few-shot learning scenario. Each incremental session adds 10 words of the corresponding session language with only 5 training samples available per word. Here we give a tutorial for the prototypical network solution (https://arxiv.org/abs/1703.05175), as presented in the NeuroBench paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the modules required for running the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from neurobench.benchmarks import Benchmark\n",
    "from neurobench.datasets import MSWC\n",
    "from neurobench.datasets.MSWC_IncrementalLoader import IncrementalFewShot\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fix the default settings. Redefine them to your liking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in repo root dir\n",
    "ROOT = \"./data/\"\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 256\n",
    "NUM_SHOTS = 5 # How many shots to use for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the desired device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device == torch.device(\"cuda\"):\n",
    "    PIN_MEMORY = True\n",
    "else:\n",
    "    PIN_MEMORY = False\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, decide if you want to go through the tutorial with the spiking neural network (SNN) or the convolutional one (CNN). Precise this by setting `SPIKING` to True (SNN) or False (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPIKING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-trained model loading\n",
    "\n",
    "We don't cover the pre-training here as it follows a standard gradient descent and can take quite some time.\n",
    "\n",
    "The pre-training step is nevertheless significant for the FSCIL performance. The models are pre-trained on the MSWC base training subset (in code: `MSWC(root=..., subset=\"base\", procedure=\"training\")`) which has 100 classes with 500 samples per class. The detailed pre-training procedure can be found in the _mswc_fscil.py_ code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from M5 import M5\n",
    "\n",
    "MODEL_SAVE_DIR = \"./model_data/\"  #Folder where pre-trained models are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the corresponding pre-trained model `mswc_rsnn_proto` (SNN) or `mswc_cnn_proto` (CNN) which are made available directly in the NeuroBench github repo under the `examples/mswc_fscil/model_data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPIKING:\n",
    "    model = SNN(\n",
    "        input_shape=(256, 201, 20),\n",
    "        neuron_type=\"RadLIF\",\n",
    "        layer_sizes=[1024, 1024, 200],\n",
    "        normalization=\"batchnorm\",\n",
    "        dropout=0.1,\n",
    "        bidirectional=False,\n",
    "        use_readout_layer=True,\n",
    "        ).to(device)\n",
    "    \n",
    "    state_dict = torch.load(os.path.join(MODEL_SAVE_DIR, \"mswc_rsnn_proto\"),\n",
    "                        map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "else:\n",
    "    model = M5(n_input=20, stride=2, n_channel=256, \n",
    "            n_output=200, input_kernel=4, pool_kernel=2, drop=True).to(device)\n",
    "\n",
    "    state_dict = torch.load(os.path.join(MODEL_SAVE_DIR, \"mswc_cnn_proto\"),\n",
    "                        map_location=device)\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the model below. As you can see:\n",
    "\n",
    "- The CNN model follows the multilayer M5 architecture defined in https://arxiv.org/abs/1610.00087 with a tuned kernel size to match the employed pre-processing.\n",
    "- The SNN model consists of 2 recurrent spiking neuron layers and a linear readout layer, adapted from the sparchSNN library https://github.com/idiap/sparch. The spiking neurons are leaky integrate and fire neurons with an extra adaptive variable to mitigate the impact of average activity. All neuron parameters are trained heterogeneously during pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M5(\n",
       "  (conv1): Conv1d(20, 256, kernel_size=(4,), stride=(2,))\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU()\n",
       "  (drop1): Dropout(p=0.2, inplace=False)\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): ReLU()\n",
       "  (drop2): Dropout(p=0.2, inplace=False)\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
       "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act3): ReLU()\n",
       "  (drop3): Dropout(p=0.2, inplace=False)\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
       "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act4): ReLU()\n",
       "  (drop4): Dropout(p=0.2, inplace=False)\n",
       "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (output): Linear(in_features=512, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we convert the model to a NeuroBench TorchModel to allow for computational metric benchmarking. This creates hooks to the model activity functions. The neural network itself is now stored in `model.net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurobench.models import TorchModel\n",
    "\n",
    "model = TorchModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For manually defined activation modules, like the adapative LIF neuron used for the SNN model, we need to add this hook manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPIKING: \n",
    "    model.add_activation_module(RadLIFLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "For the proposed solution, we employ a state-of-the-art pre-processing, namely Mel Frequency Cepstral Coefficients (MFCC) to extract relevant frequency-based coefficients. We employ the torchaudio MFCC processor (https://pytorch.org/audio/main/generated/torchaudio.transforms.MFCC.html) and tune the hop length to fix the resolution to 200Hz and the number of mel coefficients to 20 for a reasonable number of input channels to the network.\n",
    "\n",
    "For the _spiking_ solution, a delta-encoding is added on top of MFCC to convert the signals to spikes. This is done with the Speech2Spike pipeline (https://dl.acm.org/doi/abs/10.1145/3584954.3584995) that has directly been integrated in NeuroBench. We note that this adds a spiking threshold as an extra parameter. It was fixed to 1 following the Speech2Spikes initial observations here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurobench.preprocessing import MFCCPreProcessor, S2SPreProcessor\n",
    "\n",
    "n_fft = 512\n",
    "win_length = None\n",
    "hop_length = 240\n",
    "n_mels = 20\n",
    "n_mfcc = 20\n",
    "\n",
    "if SPIKING:\n",
    "    encode = S2SPreProcessor(device, transpose=True)\n",
    "    config_change = {\"sample_rate\": 48000,\n",
    "                     \"hop_length\": 240}\n",
    "    encode.configure(threshold=1.0, **config_change)\n",
    "else:\n",
    "    encode = MFCCPreProcessor(\n",
    "        sample_rate=48000,\n",
    "        n_mfcc=n_mfcc,\n",
    "        melkwargs={\n",
    "            \"n_fft\": n_fft,\n",
    "            \"n_mels\": n_mels,\n",
    "            \"hop_length\": hop_length,\n",
    "            \"mel_scale\": \"htk\",\n",
    "            \"f_min\": 20,\n",
    "            \"f_max\": 4000,\n",
    "        },\n",
    "        device = device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Prototypical Continual Learning\n",
    "\n",
    "Before we can start using the prototypical network approach for learning incremental classes, we need to align the pre-trained model with this approach. The prototypical network approach (https://arxiv.org/abs/1703.05175) indeed relies on implementing a clustering protocol, based on the pre-trained feature extractor, as a linear readout layer; but this requires all parameters of this readout layer to be defined accordingly. Thus we first redefine the readout layer for the 100 base classes following the prototypical network approach (such that they will align with the incremental classes prototypical readout parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, we load the base training dataset that is the data available to generate the prototypical representations for the base classes. If the MSWC FSCIL dataset is not already available at `ROOT`, the entire dataset will first be downloaded from _Hugging Face_ at the following address: https://huggingface.co/datasets/NeuroBench/mswc_fscil_subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_set = MSWC(root=ROOT, subset=\"base\", procedure=\"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a dataloader **without shuffling** and with a batch_size of 500, which, following the definition of the dataset, will provide all samples of 1 class at each new batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(base_train_set, batch_size=500, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prototypical readout parameters are defined based on the mean extracted feature $c_k$ from all training sample of the corresponding class $k$, which we get by passing all input samples through the backbone of the pre-trained network (all layers except the readout one). The prototypical weights and biases for class $k$ then are: $W_k = 2c_k, \\ \\ b_k=c_kc_k^T$.\n",
    "\n",
    "To do so, we first define a new readout layer supporting 200 classes (100 base classes + 100 incrementally learned classes) that will replace the pre-trained one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up new proto readout layer\n",
    "if SPIKING:\n",
    "    output = model.net.snn[-1].W\n",
    "    proto_out = nn.Linear(output.weight.shape[1], 200, bias=True).to(device)\n",
    "    proto_out.weight.data = output.weight.data\n",
    "else:\n",
    "    output = model.net.output\n",
    "    proto_out = nn.Linear(512, 200, bias=True).to(device)\n",
    "    proto_out.weight.data = output.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pass through each of the base training classes, get all of the 500 associated sample feature, average them and define the weights and biases accordingly.\n",
    "\n",
    "Just note that for the _spiking_ solution, the features are summed over time and thus the bias is also divided by the number of total timesteps.\n",
    "\n",
    "_Note_: This procedure can take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:02<00:00,  4.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Compute prototype weights for base classes\n",
    "\n",
    "for data, target in tqdm(train_loader):\n",
    "    data, target = encode((data.to(device), target.to(device)))\n",
    "    data = data.squeeze()\n",
    "    class_id = target[0]\n",
    "\n",
    "    if SPIKING:\n",
    "        features = data\n",
    "        for layer in model.net.snn[:-1]:\n",
    "            features = layer(features)\n",
    "\n",
    "        mean = torch.sum(features, dim=[0,1])/500\n",
    "        proto_out.weight.data[class_id] = 2*mean\n",
    "        proto_out.bias.data[class_id] = -torch.matmul(mean, mean.t())/features.shape[1]\n",
    "\n",
    "    else:\n",
    "        features = model.net(data, features_out=True)\n",
    "\n",
    "        mean = torch.sum(features, dim=0)/500\n",
    "        proto_out.weight.data[class_id] = 2*mean\n",
    "        proto_out.bias.data[class_id] = -torch.matmul(mean, mean.t())\n",
    "\n",
    "    del data\n",
    "    del features\n",
    "    del mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we replace the pre-trained readout layer by the newly defined prototypical one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace pre-trained readout with prototypical layer\n",
    "if SPIKING:\n",
    "    model.net.snn[-1].W = proto_out\n",
    "else:\n",
    "    model.net.output = proto_out\n",
    "\n",
    "del base_train_set\n",
    "del train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test the performance of the prototypical representations on the base test set using a NeuroBench Benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M5(\n",
       "  (conv1): Conv1d(20, 256, kernel_size=(4,), stride=(2,))\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU()\n",
       "  (drop1): Dropout(p=0.2, inplace=False)\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): ReLU()\n",
       "  (drop2): Dropout(p=0.2, inplace=False)\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
       "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act3): ReLU()\n",
       "  (drop3): Dropout(p=0.2, inplace=False)\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
       "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act4): ReLU()\n",
       "  (drop4): Dropout(p=0.2, inplace=False)\n",
       "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (output): Linear(in_features=512, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy model for evaluation\n",
    "eval_model = copy.deepcopy(model)\n",
    "\n",
    "# Get base test set for evaluation\n",
    "base_test_set = MSWC(root=ROOT, subset=\"base\", procedure=\"testing\")\n",
    "test_loader = DataLoader(base_test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "eval_model.net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As NeuroBench Benchmarks encapsulate the whole testing, it requires some pre and post-processors to manipulate data before and aftera network pass. We thus define the following utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeeze = lambda x: (x[0].squeeze(), x[1])\n",
    "out2pred = lambda x: torch.argmax(x, dim=-1)\n",
    "to_device = lambda x: (x[0].to(device), x[1].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a mask function for this evaluation as the network is directly defined with 200 output neurons but we are for now evaluating the performance solely on the 100 base classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific post-processing with masking on the base classes\n",
    "mask = torch.full((200,), float('inf')).to(device)\n",
    "mask[torch.arange(0,100, dtype=int)] = 0\n",
    "out_mask = lambda x: x - mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the Benchmark object with the desired metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "static_metrics = [\"footprint\", \"connection_sparsity\"]\n",
    "workload_metrics = [\"classification_accuracy\", \"activation_sparsity\", \"synaptic_operations\"]\n",
    "\n",
    "# Define benchmark object\n",
    "benchmark_all_test = Benchmark(eval_model, metric_list=[static_metrics, workload_metrics], \n",
    "                               dataloader=test_loader, \n",
    "                               preprocessors=[to_device, encode, squeeze], postprocessors=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the Benchmark on the base test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/home/tanvirharris/anaconda3/lib/python3.11/site-packages/neurobench/benchmarks/benchmark.py:84: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds = alg(preds)\n",
      "100%|██████████| 40/40 [01:48<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base results: {'footprint': 6028096, 'connection_sparsity': 0.0, 'classification_accuracy': 0.9471999999999999, 'activation_sparsity': 0.7841849379595587, 'synaptic_operations': {'Effective_MACs': 7810468.6368, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
      "The base accuracy is 94.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pre_train_results = benchmark_all_test.run(postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
    "\n",
    "print(\"Base results:\", pre_train_results)\n",
    "\n",
    "print(f\"The base accuracy is {pre_train_results['classification_accuracy']*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the performance of session 0.\n",
    "\n",
    "Note that the obtained accuracy, after conversion to prototypes, is below the original performance of the pre-trained model. This is a price to pay to allow for the prototypical network to work effectively in the incremental sessions. This could nevertheless still be improved upon, especially for the _spiking_ solution, where the conversion accuracy drop is significant (from 93% to 84%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Learning\n",
    "\n",
    "We can now pursue with the few-shot incremental sessions. New sessions are learned following the prototypical network approach on the corresponding session classes and with the limited number of samples available.\n",
    "\n",
    "We first initialize the FSCIL dataloader. It will generate 10 sessions from a random ordering of the 10 incremental languages. Each session consists of\n",
    "- One `support` _list_ of `NUM_SHOTS` shots, each shot being a tuple of tensors `(X_shot, y_shot)` with one sample for each of the 10 session classes.  \n",
    "- One `query` _dataset_ with all the current and prior incremental session classes and `query_shots` samples per class.\n",
    "- One `query_classes` list that contains each unique incremental class index following their order of appearance.\n",
    "\n",
    "Note that the `support_query_split` is here to define a pre-sampling split between samples available for support and for query in this order. In the proposed set-up, the few-shot dataloader thus fixes the 100 query samples per class from the start and samples 5 shots out of a 100 samples for each incremental class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IncrementalFewShot Dataloader used in incremental mode to generate class-incremental sessions\n",
    "few_shot_dataloader = IncrementalFewShot(k_shot=NUM_SHOTS, \n",
    "                            root = ROOT,\n",
    "                            query_shots=100,\n",
    "                            support_query_split=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run one incremental session learning as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "support, query, query_classes = next(iter(few_shot_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support data - which is generated in a shot-by-shot way for universality to different methods - is here concatenated to gather all training samples per class for the prototypical approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "\n",
    "for X_shot, y_shot in support:\n",
    "    if data is None:\n",
    "        data = X_shot\n",
    "        target = y_shot\n",
    "    else:\n",
    "        data = torch.cat((data,X_shot), 0)\n",
    "        target = torch.cat((target,y_shot), 0)\n",
    "\n",
    "data, target = encode((data.to(device), target.to(device)))\n",
    "data = data.squeeze()\n",
    "\n",
    "new_classes = y_shot.tolist()\n",
    "Nways = len(y_shot) # Number of ways of one batch, should always be 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the prototypical network approach on the corresponding incremental classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPIKING:\n",
    "    features = eval_model.net.snn[0](data)\n",
    "    features = eval_model.net.snn[1](features)\n",
    "\n",
    "    for index, class_id in enumerate(new_classes):\n",
    "        mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=[0,1])/NUM_SHOTS\n",
    "        eval_model.net.snn[-1].W.weight.data[class_id] = 2*mean\n",
    "        eval_model.net.snn[-1].W.bias.data[class_id] = -torch.matmul(mean, mean.t())/(features.shape[1])\n",
    "else:\n",
    "    features = eval_model.net(data, features_out=True)\n",
    "\n",
    "    for index, class_id in enumerate(new_classes):\n",
    "        mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=0)/NUM_SHOTS\n",
    "        eval_model.net.output.weight.data[class_id] = 2*mean\n",
    "        eval_model.net.output.bias.data[class_id] = -torch.matmul(mean, mean.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we evaluate the performance after one FSCIL session. The default FSCIL benchmarking evaluates accuracy on all classes seen so far, including the base classes used for pre-training. To this, we add an evaluation of the performance solely on the incremental few-shot classes, corresponding to only the `query` dataset.\n",
    "\n",
    "Note that the dataloaders used for the benchmarking are actually redefined when running the Benchmark object. This is to be aligned with the general case of multiple sessions (see cell below) as the data to test on changes over sessions in a FSCIL task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:11<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on new classes: 86.69999976158142 %\n",
      "Running benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [02:01<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session accuracy: 93.70909091776066 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define benchmark object for incremental classes\n",
    "benchmark_new_classes = Benchmark(eval_model, metric_list=[[],[\"classification_accuracy\"]], \n",
    "                                  dataloader=None,\n",
    "                                  preprocessors=[to_device, encode, squeeze], postprocessors=[])    \n",
    "\n",
    "### Testing phase ###\n",
    "eval_model.net.eval()\n",
    "\n",
    "# Define session dataloaders for query and query + base_test samples\n",
    "query_loader = DataLoader(query, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "full_session_test_set = ConcatDataset([base_test_set, query])\n",
    "full_session_test_loader = DataLoader(full_session_test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Create a mask function to only consider accuracy on classes presented so far\n",
    "session_classes = torch.cat((torch.arange(0,100, dtype=int), torch.IntTensor(query_classes))) \n",
    "mask = torch.full((200,), float('inf')).to(device)\n",
    "mask[session_classes] = 0\n",
    "out_mask = lambda x: x - mask\n",
    "\n",
    "\n",
    "# Run benchmark on query classes only\n",
    "query_results = benchmark_new_classes.run(dataloader = query_loader, \n",
    "                                          postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
    "print(f\"Accuracy on new classes: {query_results['classification_accuracy']*100} %\")\n",
    "\n",
    "# Run benchmark to evaluate accuracy of this specific session\n",
    "session_results = benchmark_all_test.run(dataloader = full_session_test_loader, \n",
    "                                         postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
    "print(f\"Session accuracy: {session_results['classification_accuracy']*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the full FSCIL setup by looping over the code as presented above for all 10 sessions:\n",
    "\n",
    "_Note_: This can take a bit of time as FSCIL requires for increasingly heavy datasets to be loaded in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session: 1\n",
      "Running benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on new classes: 86.59999990463257 %\n",
      "Running benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [02:37<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session accuracy: 93.09090912558815 %\n",
      "Session results: {'footprint': 6028096, 'connection_sparsity': 0.0002, 'classification_accuracy': 0.9309090912558815, 'activation_sparsity': 0.7856132098791222, 'synaptic_operations': {'Effective_MACs': 7766932.573818182, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
      "Session: 2\n",
      "Running benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:19<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on new classes: 88.65000014305114 %\n",
      "Running benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 18/47 [01:21<02:11,  4.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy on new classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m %\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Run benchmark to evaluate accuracy of this specific session\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m session_results \u001b[38;5;241m=\u001b[39m benchmark_all_test\u001b[38;5;241m.\u001b[39mrun(dataloader \u001b[38;5;241m=\u001b[39m full_session_test_loader, postprocessors\u001b[38;5;241m=\u001b[39m[out_mask, F\u001b[38;5;241m.\u001b[39msoftmax, out2pred, torch\u001b[38;5;241m.\u001b[39msqueeze])\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m %\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, session_results)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neurobench/benchmarks/benchmark.py:89\u001b[0m, in \u001b[0;36mBenchmark.run\u001b[0;34m(self, quiet, verbose, dataloader, preprocessors, postprocessors)\u001b[0m\n\u001b[1;32m     87\u001b[0m batch_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkload_metrics\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 89\u001b[0m     batch_results[m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkload_metrics[m](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, preds, data)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m, v \u001b[38;5;129;01min\u001b[39;00m batch_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# AccumulatedMetrics are computed after all batches complete\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkload_metrics[m], workload_metrics\u001b[38;5;241m.\u001b[39mAccumulatedMetric):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neurobench/benchmarks/workload_metrics.py:206\u001b[0m, in \u001b[0;36msynaptic_operations.__call__\u001b[0;34m(self, model, preds, data)\u001b[0m\n\u001b[1;32m    204\u001b[0m     spikes \u001b[38;5;241m=\u001b[39m spikes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    205\u001b[0m hook\u001b[38;5;241m.\u001b[39mhook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[0;32m--> 206\u001b[0m operations, spiking \u001b[38;5;241m=\u001b[39m single_layer_MACs(spikes, hook\u001b[38;5;241m.\u001b[39mlayer)\n\u001b[1;32m    207\u001b[0m total_ops,_ \u001b[38;5;241m=\u001b[39m single_layer_MACs(spikes, hook\u001b[38;5;241m.\u001b[39mlayer, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_synops \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_ops\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neurobench/utils.py:181\u001b[0m, in \u001b[0;36msingle_layer_MACs\u001b[0;34m(inputs, layer, total)\u001b[0m\n\u001b[1;32m    178\u001b[0m macs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# copy input\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m inputs, spiking, in_states \u001b[38;5;241m=\u001b[39m binary_inputs(inputs, all_ones\u001b[38;5;241m=\u001b[39mtotal)\n\u001b[1;32m    183\u001b[0m stateless_layers \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv2d, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv1d, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv3d)\n\u001b[1;32m    184\u001b[0m recurrent_layers \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mRNNBase)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neurobench/utils.py:164\u001b[0m, in \u001b[0;36mbinary_inputs\u001b[0;34m(inputs, all_ones)\u001b[0m\n\u001b[1;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    163\u001b[0m in_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs[(inputs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (inputs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (inputs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    165\u001b[0m     spiking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m inputs[inputs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:996\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;21m__neg__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mTensorBase\u001b[38;5;241m.\u001b[39mneg\n\u001b[1;32m    994\u001b[0m \u001b[38;5;21m__abs__\u001b[39m \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mTensorBase\u001b[38;5;241m.\u001b[39mabs\n\u001b[0;32m--> 996\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iteration over incremental sessions\n",
    "for session, (support, query, query_classes) in enumerate(few_shot_dataloader):\n",
    "    print(f\"Session: {session+1}\")\n",
    "\n",
    "    ### Computing new Prototypical Weights ###\n",
    "    data = None\n",
    "    \n",
    "    for X_shot, y_shot in support:\n",
    "        if data is None:\n",
    "            data = X_shot\n",
    "            target = y_shot\n",
    "        else:\n",
    "            data = torch.cat((data,X_shot), 0)\n",
    "            target = torch.cat((target,y_shot), 0)\n",
    "\n",
    "    data, target = encode((data.to(device), target.to(device)))\n",
    "    data = data.squeeze()\n",
    "\n",
    "    new_classes = y_shot.tolist()\n",
    "    Nways = len(y_shot) # Number of ways, should always be 10\n",
    "\n",
    "    if SPIKING:\n",
    "        features = eval_model.net.snn[0](data)\n",
    "        features = eval_model.net.snn[1](features)\n",
    "\n",
    "        for index, class_id in enumerate(new_classes):\n",
    "            mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=[0,1])/NUM_SHOTS\n",
    "            eval_model.net.snn[-1].W.weight.data[class_id] = 2*mean\n",
    "            eval_model.net.snn[-1].W.bias.data[class_id] = -torch.matmul(mean, mean.t())/(features.shape[1])\n",
    "    else:\n",
    "        features = eval_model.net(data, features_out=True)\n",
    "\n",
    "        for index, class_id in enumerate(new_classes):\n",
    "            mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=0)/NUM_SHOTS\n",
    "            eval_model.net.output.weight.data[class_id] = 2*mean\n",
    "            eval_model.net.output.bias.data[class_id] = -torch.matmul(mean, mean.t())\n",
    "\n",
    "    ### Testing phase ###\n",
    "    eval_model.net.eval()\n",
    "\n",
    "    # Define session dataloaders for query and query + base_test samples\n",
    "    query_loader = DataLoader(query, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    full_session_test_set = ConcatDataset([base_test_set, query])\n",
    "    full_session_test_loader = DataLoader(full_session_test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Create a mask function to only consider accuracy on classes presented so far\n",
    "    session_classes = torch.cat((torch.arange(0,100, dtype=int), torch.IntTensor(query_classes))) \n",
    "    mask = torch.full((200,), float('inf')).to(device)\n",
    "    mask[session_classes] = 0\n",
    "    out_mask = lambda x: x - mask\n",
    "\n",
    "    # Run benchmark on query classes only\n",
    "    query_results = benchmark_new_classes.run(dataloader = query_loader, postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
    "    print(f\"Accuracy on new classes: {query_results['classification_accuracy']*100} %\")\n",
    "\n",
    "    # Run benchmark to evaluate accuracy of this specific session\n",
    "    session_results = benchmark_all_test.run(dataloader = full_session_test_loader, postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
    "    print(f\"Session accuracy: {session_results['classification_accuracy']*100} %\")\n",
    "    print(\"Session results:\", session_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should obtain a performance within the bounds presented in the results plot below. The shaded area represents $5^{th}$ and $95^{th}$ percentile on 100 runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/FSCIL_proto_results.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
